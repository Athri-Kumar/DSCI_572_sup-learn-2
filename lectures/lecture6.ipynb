{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI 572 \"lecture\" 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plan\n",
    "\n",
    "- T/F (15 min)\n",
    "- Quiz recap (10 min)\n",
    "- Lab 3 recap (10 min)\n",
    "- T/F (15 min)\n",
    "- Break (5 min)\n",
    "\n",
    "- T/F (15 min)\n",
    "- Neural network as a feature extractor + linear model (10 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation, Conv2D, MaxPooling2D\n",
    "from keras.models import Sequential, Model, clone_model\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosave disabled\n"
     ]
    }
   ],
   "source": [
    "%autosave 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More true/false - left over from last time (15 min)\n",
    "\n",
    "1. Each iteration of stochastic gradient is faster than each iteration of gradient descent, but we likely need more iterations in total.\n",
    "2. One epoch of stochastic gradient takes about the same amount of time as one iteration of gradient descent.\n",
    "3. Stochastic gradient with a minibatch size of $n$ is the same thing as gradient descent.\n",
    "4. In terms of the fundamental tradeoff, more layers and larger layers both lead to lower training error in a neural network.\n",
    "\n",
    "<br><br><br><br><br><br><br><br>\n",
    "\n",
    "Extra note about SGD: \n",
    "\n",
    "- We don't use the same termination condition as gradient descent, for several reasons\n",
    "  - Slow to check the full gradient\n",
    "  - Function is non-convex\n",
    "- So we just want to specify a number of iterations\n",
    "- Is 1000 iterations a good number? It depends on the data set size and minibatch size.\n",
    "- Hence we measure in epochs instead of iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz recap (10 min)\n",
    "\n",
    "See [quiz solutions](https://github.ubc.ca/MDS-2018-19/DSCI_572_sup-learn-2_students/blob/master/solutions/quiz1/quiz1.md)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 3 Exercise 0 recap (5 min)\n",
    "\n",
    "- _go through lab exercise briefly_\n",
    "\n",
    "Extra note about optimization for a neural net:\n",
    "\n",
    "- From the optimization perspective, we should think if _flattening_ all the parameters into one big vector. \n",
    "- For example if the parameters are\n",
    "\n",
    "\n",
    "$$\\begin{align}W^{(0)} &= \\begin{bmatrix}-2 & 2 & -1\\\\-1 & -2 & 0\\end{bmatrix},  &b^{(0)}&=\\begin{bmatrix}2 \\\\ 0\\end{bmatrix} \\\\ W^{(1)} &= \\begin{bmatrix}3 & 1\\end{bmatrix},  &b^{(1)}&=-10\\end{align}$$\n",
    "\n",
    "then we can flatten the whole thing down into\n",
    "\n",
    "$$w_\\text{all} = \\begin{bmatrix}W^{(0)}_\\text{flattened}\\\\b^{(0)}_\\text{flattened}\\\\ W^{(1)}_\\text{flattened}\\\\b^{(1)}_\\text{flattened}\\end{bmatrix} = \\begin{bmatrix}-2\\\\-1\\\\2\\\\-2\\\\-1\\\\0\\\\2\\\\0\\\\3\\\\1\\\\-10\\end{bmatrix}$$\n",
    "\n",
    "- And then we consider $f(w_\\text{all})$, and $\\nabla f(w_\\text{all})$, etc.\n",
    "- An implementation may or may not actually do this, but you can think of this way conceptually.\n",
    "- If you wanted to use an opimizer like `scipy.optimize.minimize` (not recommended for neural nets!) that expects vectors, then you'd need to implement that flattening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True/false - convolutions (15 min)\n",
    "\n",
    "\n",
    "\n",
    "1. Convolving with the filter $w=[0.2, 0.2, 0.2, 0.2, 0.2]$ performs a local averaging operation.\n",
    "2. For a convolution to be valid, the values in the filter must add up to $1$ (e.g. $w=[0,1,0], w=\\left[\\frac13,\\frac13,\\frac13\\right]$, etc).\n",
    "3. 1D convolution is just a special case of 2D convolution where one of the dimensions of $w$ and $x$ happens to be 1.\n",
    "4. The convolution of a filter $w$ and signal $x$ is equivalent to the matrix multiplication $w^Tx$.\n",
    "\n",
    "<br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Break (5 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution notes (0 min)\n",
    "\n",
    "- What is a convolution?\n",
    "- What is a linear operator/transformation?\n",
    "  - in the discrete world, anything that can be represented as a matrix multiplication\n",
    "  - we have linear operatiors in the continuous world (like sum, derivative; hence the trick in the last lecture), but that is out of scope here\n",
    "  - finite differencing is a linear operator. PCA is linear. \n",
    "- 1-D convolutions\n",
    "- 2-D and higher-D convolutions\n",
    "- boundary conditions: the output might be slightly bigger or slightly smaller, or the same\n",
    "  - these are the options in [scipy.signal.convolve2d](https://docs.scipy.org/doc/scipy-0.16.0/reference/generated/scipy.signal.convolve2d.html)\n",
    "\n",
    "- (optional) all of this extends to continuous scenarios, where convolution is an integral instead of a sum\n",
    "- convolution is often written as $x \\ast y$ \n",
    "- convolution is commutative: $x \\ast y = y \\ast x$\n",
    "  - Nonetheless, we often have a \"signal\" and a \"filter\" and they have different interpretations\n",
    "  - The filter is often small and has an interpretation like \"highpass\" or \"lowpass\"\n",
    "\n",
    "- Note (for completeness, not required): there is a fast implementations of convolution using the FFT (fast Fourier transform). For an $n\\times n$ image and $m\\times m$ filter this takes $nm\\log(nm)$ time instead of $n^2m^2$. For sufficiently big images and filters this is a big win BUT there's a catch, which is that the equivalence only holds for periodic boundary conditions. \n",
    "\n",
    "- Convolutions appear all over the field of _signal processing_, which is traditionally a discipline within electrical engineering. But they also show up in math, physics, CS, etc. A lot of communications theory is based on this stuff.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True/false - CNNs (15 min)\n",
    "\n",
    "1. Convolutional neural network = convolutional network = convolutional net = convnet = CNN\n",
    "2. Convolutional neural networks are a special case of fully-connected (\"regular\") neural networks where some of the weights are fixed at 0 and some of the weights are \"tied\" (fixed to be the same value).\n",
    "3. The term \"convolutional neural network\" refers specifically to using **2D** convolutions, and the main application is image data.\n",
    "4. A convnet applied to image recognition would typically have fewer parameters than a fully-connected net applied to the same problem.\n",
    "\n",
    "<br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## CNNs notes (0 min)\n",
    "\n",
    "- Imagine an neural net with image as inputs. For a $1000\\times 1000$ image that is 1 million features.\n",
    "- Now say the next layer is 10% or even 1% that size. Now we need a matrix of size $10^6\\times10^4=10^{10}$. That is not going to happen.\n",
    "- Key insight: things happen \"locally\" in images. The top-left matters and the bottom-right matters, but they don't necessarily need to interact right away.\n",
    "  - so we do some \"local processing\" on the different parts of the image, and then \"report back\" and start merging the information when we've reduced the dimension\n",
    "  - this is the promise/dream of \"deep learning\": hierarchical abstractions like edges, curves, objects, higher and higher level \"understanding\"\n",
    "- Key idea: use layers that are not fully connected (this was called \"Dense\" in Keras). Instead, have units in layer 2 that only get input from some _nearby units_ (pixels) in layer 1. \n",
    "- The above notion is precisely a convolution. Thus people talk about convolutions but keep in mind it's just a not-fully-connected neural network. This means everything from before (gradients, tricks) carry over nicely. \n",
    "- But for computational reasons we don't form those giant matrices full of zeros! We just do convolutions. \n",
    "- The parameters (weights) are now the filters themselves. So we can interpret it as \"learning the filters\". It's all the same stuff. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network as a feature extrator (10 min)\n",
    "\n",
    "Neural nework as a feature extractor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype('float32')\n",
    "\n",
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255.\n",
    "X_test = X_test / 255.\n",
    "\n",
    "# one hot encode outputs\n",
    "Y_train = np_utils.to_categorical(y_train)\n",
    "Y_test = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 10)        260       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 10)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 8, 10)          2510      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 10)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                8050      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 11,330\n",
      "Trainable params: 11,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn = Sequential()\n",
    "cnn.add(Conv2D(10, (5, 5), input_shape=(28, 28, 1), \n",
    "               activation='relu'))\n",
    "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn.add(Conv2D(10, (5, 5), activation='relu'))\n",
    "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn.add(Flatten())\n",
    "cnn.add(Dense(50, activation='relu'))\n",
    "cnn.add(Dense(10, activation='softmax'))\n",
    "\n",
    "cnn.compile(loss='categorical_crossentropy', \n",
    "            optimizer='adam', metrics=['accuracy'])\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 46s 762us/step - loss: 0.2285 - acc: 0.9315\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 44s 734us/step - loss: 0.0743 - acc: 0.9774\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 48s 804us/step - loss: 0.0523 - acc: 0.9834\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 47s 776us/step - loss: 0.0424 - acc: 0.9867\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 47s 785us/step - loss: 0.0365 - acc: 0.9885\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a1dc291d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(X_train, Y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = Model(cnn.input, cnn.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1_input (InputLayer)  (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 10)        260       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 10)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 8, 10)          2510      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 10)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                8050      \n",
      "=================================================================\n",
      "Total params: 10,820\n",
      "Trainable params: 10,820\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "feature_extractor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_train = feature_extractor.predict(X_train)\n",
    "Z_test  = feature_extractor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 50)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mgelbart/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='sag',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(multi_class=\"multinomial\", solver=\"sag\", C=1000)\n",
    "lr.fit(Z_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 15s 245us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.03379892049527843, 0.9889166666666667]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.evaluate(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99455"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(Z_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(cnn.predict(X_test[:15]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1], dtype=uint8)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.predict(Z_test[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we can see, the predictions are the same. \n",
    "- The probabilities and coefficients aren't exactly the same (it's hard to match everything up perfectly)\n",
    "- If we want to make them exactly the same, we can set the coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 10)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = cnn.layers[-1].get_weights()[0]\n",
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = cnn.layers[-1].get_weights()[1]\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 50)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.coef_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.intercept_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.coef_ = W.T\n",
    "lr.intercept_ = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.2994968e-08, 1.8788144e-06, 1.1874030e-07, 1.2116589e-05,\n",
       "        2.8262336e-12, 4.1235131e-07, 2.3576339e-13, 9.9996102e-01,\n",
       "        5.2370819e-08, 2.4371320e-05]], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.predict(X_test[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.29949935e-08, 1.87881642e-06, 1.18740431e-07, 1.21165895e-05,\n",
       "        2.82624466e-12, 4.12351739e-07, 2.35763422e-13, 9.99961138e-01,\n",
       "        5.23707229e-08, 2.43713221e-05]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.predict_proba(Z_test[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bam!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
