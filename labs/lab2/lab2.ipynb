{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI 572 Lab 2\n",
    "\n",
    "#### Extra dependencies:\n",
    "\n",
    "- autograd: `pip install autograd`\n",
    "\n",
    "#### Notes: \n",
    "\n",
    "- This lab has a lot of questions, but many are optional.\n",
    "- Roughly speaking, Exercises 1-4 pertain to Monday's lecture, and the rest to Wednesday's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd \n",
    "from autograd import elementwise_grad as egrad  # for functions that vectorize over inputs\n",
    "from autograd import grad\n",
    "import autograd.numpy as np\n",
    "\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import scipy.optimize as spo\n",
    "import scipy.special\n",
    "import sklearn.datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "rubric={mechanics:20}\n",
    "\n",
    "Follow the [general lab instructions](https://ubc-mds.github.io/resources_pages/general_lab_instructions/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: floating point errors\n",
    "\n",
    "For each of the code snippets below, explain the result. The first three are answered for you, so you can get a sense of what types of explanations we're looking for. Note that in both the second and third case we get the \"expected\" result (zero), but the explanations are very different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.7755575615628914e-17"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.3 - 0.2 - 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample solution**: The result is not zero because 0.3, 0.2, and 0.1 are not represented exactly as floating point numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.5 - 0.25 - 0.125 - 0.125"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample solution**: The result is zero because 0.5, 0.25, and 0.125 are powers of 2 and there they _are_ represented exactly as floating point numbers. There is no roundoff error present. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.4 - 0.2 - 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample solution**: The result is correct (zero) despite the fact that 0.4 and 0.2 are not represented exactly as floating point numbers. This is a case of good luck: while 0.4 and 0.2 are not represented exactly, the roundoff errors happened to cancel out during the subtractions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### snippet (a)\n",
    "rubric={reasoning:5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "30 - 20 - 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### snippet (b)\n",
    "rubric={reasoning:5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "30.0 - 20.0 - 10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### snippet (c)\n",
    "rubric={reasoning:5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(10.0**100 + 1) == 10.0**100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### snippet (d)\n",
    "rubric={reasoning:5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "OverflowError",
     "evalue": "(34, 'Result too large')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-d9598508f970>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0;36m10.0\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m100000\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m10.0\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mOverflowError\u001b[0m: (34, 'Result too large')"
     ]
    }
   ],
   "source": [
    "(10.0**100000 + 1) - 10.0**100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### snippet (e)\n",
    "rubric={reasoning:5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/autograd/tracer.py:48: RuntimeWarning: overflow encountered in exp\n",
      "  return f_raw(*args, **kwargs)\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(1000) - np.exp(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### snippet (f)\n",
    "rubric={reasoning:5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/autograd/tracer.py:48: RuntimeWarning: overflow encountered in exp\n",
      "  return f_raw(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/np.exp(1000) == np.exp(-1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### snippet (g)\n",
    "rubric=reasoning(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/np.exp(100) == np.exp(-100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### snippet (h)\n",
    "rubric=reasoning(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/autograd/tracer.py:48: RuntimeWarning: overflow encountered in exp\n",
      "  return f_raw(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(1000)==np.exp(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### snippet (i)\n",
    "rubric={reasoning:5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999999"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.zeros(10)+0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### snippet (j)\n",
    "rubric={reasoning:5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2246467991473532e-16"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sin(np.pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (optional) snippet (k)\n",
    "rubric={reasoning:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.ones(100000)\n",
    "x[0] = 1e20\n",
    "\n",
    "y = np.ones(100000)\n",
    "y[-1] = 1e20\n",
    "\n",
    "sum(x) == sum(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (optional) snippet (l)\n",
    "rubric={reasoning:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "f = lambda x: np.sqrt(1+x**2)\n",
    "g = lambda x: x * np.sqrt(1+1/x**2)\n",
    "\n",
    "print(f(10)  == g(10))\n",
    "print(f(100) == g(100))\n",
    "print(f(300) == g(300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (optional) snippet (m)\n",
    "rubric={reasoning:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.zeros(10)+0.1\n",
    "sum(x) == np.sum(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint for the above: see [Pairwise summation](https://en.wikipedia.org/wiki/Pairwise_summation). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: floating point max/min\n",
    "rubric={reasoning:20}\n",
    "\n",
    "As discussed in lecture, IEEE double precision floating point numbers use 53 bits for the mantissa (one of which is a sign bit) and 11 bits for the exponent (again, one of which is a sign bit). Given thus, calculate the largest (furthest from zero) and smallest (closest to zero) possible representable floating point numbers. Then empirically check your results with Python. Are they what you expected? Discuss.\n",
    "\n",
    "NOTE: Python has a special behaviour that we need to watch out for. If you do something like `10**1000` you will get a giant integer. That's because Python has a dynamically expanding integer type. This has nothing to do with floating point representations, which are the thing we really care about for scientific computation (not to mention that most other languages, including R, do not do this). So, when playing around, make sure you write `10**1000.0` or `10.0*1000` to ensure it's a floating point. Or `1e1000`... but that doesn't work for other bases besides 10. \n",
    "\n",
    "(Also, and this is _REALLY_ out of scope but just FYI if anyone cares, in some languages `1eX` and `10^X` will return slightly different answers, if the language uses a special routine for `1eX` that is more optimized than the generral power function. I cannot imagine this ever mattering to any of us.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (optional) Exercise 3: $\\log(1+\\exp(z))$, continued\n",
    "rubric={accuracy:1,reasoning:1}\n",
    "\n",
    "In lecture we discussed computing $\\log(1+\\exp(z))$. We wrote a better version of the function for when $z>>1$, reproduced below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_1_plus_exp(z):\n",
    "    return np.log(1+np.exp(z))\n",
    "\n",
    "def log_1_plus_exp_safe(z):\n",
    "    if z > 100:\n",
    "        return z\n",
    "    else:\n",
    "        return np.log(1+np.exp(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's consider the case of $z\\ll -1$, i.e. when $z$ is a large negative number. In that case, we get underflow with both of the above implementations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(log_1_plus_exp(-100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(log_1_plus_exp_safe(-100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your tasks:\n",
    "\n",
    "1. Investigate why this is happening. Is the problem that $\\exp(-100)$ itself underflows?\n",
    "2. Write a function `log_1_plus_exp_safer` that works well when $z\\gg 1$ and $z\\ll -1$.\n",
    "3. For what range of values of $z$ does your `log_1_plus_exp_safer` function give reasonable results?\n",
    "4. Your code presumably contains two thresholds, the upper and lower cutoffs for $z$ at which the approximations are invoked. Can you reason about the \"optimal\" or \"best\" values for these thresholds?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: softmax logistic regression and log-sum-exp\n",
    "rubric={reasoning:15,accuracy:5}\n",
    "\n",
    "In the \"multinomial\" (aka softmax) approach to multi-class logistic regression, your loss ends up having one term for each class, so you get something of the form $\\log \\sum_{i=1}^n \\exp(x_i)$. We can rewrite this as \n",
    "\n",
    "$$\\log \\displaystyle\\sum_{i=1}^n \\exp(x_i) = a+ \\log \\displaystyle\\sum_{i=1}^n \\exp(x_i-a)$$\n",
    "\n",
    "for any $a$. Make sure you understand why this is the case before proceeding.\n",
    "\n",
    "1. Explain why this formulation might be more numerically stable and why $a=\\max \\{x_1,x_2,\\ldots,x_n\\}$ is a sensible choice.\n",
    "2. If $a=\\max \\{x_1,x_2,\\ldots,x_n\\}$, this trick seems to rely on the fact that overflow is more of a danger than underflow, because we may now compute $\\exp(z)$ for $z\\gg 1$. Explain why overflow is more of a problem than underflow here.\n",
    "3. (optional) Earlier, we used the approximation $\\log(1+\\exp(x))\\approx x$ when $x\\gg 1$. Is that just a specific case of what we have here? It seems that what we did earlier was an approximation, whereas what we did here is mathematically exact. Do you think there is any significance to this distinction, in practice?\n",
    "4. Write a python function `log_sum_exp` that takes in an array `x` and computes the above expression. Try it out for a few values: does it help with the overflow problems? Also, compare your implementation to In fact, [`scipy.special.logsumexp`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.logsumexp.html#scipy.special.logsumexp) for one or two cases.\n",
    "\n",
    "(FYI: you'll see this trick in real implementations of ML algorithms! )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: finite differences\n",
    "\n",
    "### 5(a) Optimization without a gradient: experiments\n",
    "rubric={reasoning:10}\n",
    "\n",
    "The `scipy.optimize.minimize` function takes in the derivative information through the optional argument `jac` (for Jacobean). As shown below, the code works even if you don't pass in the derivative. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun(x):\n",
    "    return (x[0]-2)**2 + (x[1]+4)**4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.99999996, -3.98923058])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spo.minimize(fun, np.zeros(2)).x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "When the gradient is not provided, `scipy.optimize.minimize` uses finite differences to estimate the gradient.\n",
    "\n",
    "In lab 1, you implemented logistic regression \"from scratch\" using `scipy.optimize.minimize`, passing in the gradient. Modify your code so that the gradient is not passed in. Compare the accuracy and speed to your implementation from lab 1. We'll use a slightly smaller version of the dataset from lab 1, with 2000 examples and 100 feature, prepared below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_df = pd.read_csv('../lab1/imdb_master.csv', encoding = \"ISO-8859-1\")\n",
    "\n",
    "# Only keep the reviews with pos and neg labels\n",
    "imdb_df = imdb_df[imdb_df['label'].str.startswith(('pos','neg'))]\n",
    "\n",
    "# Train/test split\n",
    "imdb_df_train = imdb_df[imdb_df[\"type\"] == \"train\"]\n",
    "imdb_df_test  = imdb_df[imdb_df[\"type\"] == \"test\"]\n",
    "\n",
    "# Sample 5000 rows from the dataframe. \n",
    "imdb_df_subset_train = imdb_df_train.sample(n=2000, random_state=0)\n",
    "imdb_df_subset_test  = imdb_df_test.sample(n=2000, random_state=0)\n",
    "\n",
    "# Vectorizer\n",
    "movie_vec = CountVectorizer(max_features=100, stop_words='english', binary = True)\n",
    "movie_vec.fit(imdb_df_subset_train['review'])\n",
    "\n",
    "# Create X and y\n",
    "X_train = movie_vec.transform(imdb_df_subset_train['review']).toarray() \n",
    "y_train = (imdb_df_subset_train.label.values == \"pos\")*2-1\n",
    "\n",
    "X_test = movie_vec.transform(imdb_df_subset_test['review']).toarray() \n",
    "y_test = (imdb_df_subset_test.label.values == \"pos\")*2-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5(b) Optimization without a gradient: pondering\n",
    "rubric=reasoning{10}\n",
    "\n",
    "The code should be slower when you don't provide the gradient. How much slower do you expect it to be, in terms of the size of the data set? Does theory align with experiment here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5(c) Gradient checkers\n",
    "rubric={reasoning:20}\n",
    "\n",
    "One big issue with implementing the gradient by hand is that we often make mistakes. In Lab 1 you were given the following code, which calls the \"gradient checker\" from `scipy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_grad_and_print(fun, fun_grad, dims=5):\n",
    "    x0 = np.random.rand(dims)\n",
    "    diff = spo.check_grad(fun, fun_grad, np.array(x0))\n",
    "    if diff < 1e-5:\n",
    "        print('Success (probably)')\n",
    "    else:\n",
    "        print('Gradient incorrect (probably)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under what circumstances might the gradient checker not work as intended, or give misleading results? Find a \"false positive\" and a \"false negative\", i.e. a case in which the gradient is correct but your gradient checker thinks the gradient is wrong, and a case in which the gradient is wrong but your gradient checker test passes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (optional) 5(d): implementing a gradient checker\n",
    "rubric={accuracy:1,quality:1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier, you used the [scipy gradient checker](https://docs.scipy.org/doc/scipy-0.17.0/reference/generated/scipy.optimize.check_grad.html). Implement your own gradient checker, that has the same interface as the scipy one (i.e. takes arguments `func`, `grad`, `x0` and returns the norm of the difference.) So long as you **cite your sources**, you may refer to, or even take code from, the lecture notes and/or the [scipy gradient checker source code](https://github.com/scipy/scipy/blob/v0.17.0/scipy/optimize/optimize.py#L625-L671) and/or the [scipy finite differences source code](https://github.com/scipy/scipy/blob/v0.18.1/scipy/optimize/optimize.py#L633-L688).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6: AutoGrad\n",
    "\n",
    "### 6(a) Running AutoGrad\n",
    "rubric={accuracy:5,quality:5}\n",
    "\n",
    "Install [AutoGrad](https://github.com/HIPS/autograd) using `pip install autograd`. Then, make a plot like the `tanh` example on the AutoGrad README, but with a different (differentiable) function of your choosing. To make things easier, I suggest a scalar function, so that we're dealing with derivatives rather than gradients (though AutoGrad can handle gradients too). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (optional) 6(b): AutoGrad for logistic regression\n",
    "rubric={reasoning:1}\n",
    "\n",
    "In lab 1, you fit parameters for logistic regression using a gradient function that was derived by hand. Here, copy over the loss function `loss_lr` and use AutoGrad to handle the gradient. Compare the result to `loss_lr_grad` from lab 1. Does it work, i.e. does it give you the same gradient? You can generate some random training data for this experiment.\n",
    "\n",
    "Note: in real situations, you would need to worry about the numerical issues discussed in Exercise 4, since autograd uses the loss and the loss isn't implemented in a stable way. You can solve the problem by using `autograd.scipy.misc.logsumexp`. However, you shouldn't need that here."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
