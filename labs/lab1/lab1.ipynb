{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI 572 Lab 1\n",
    "Meta-comment: in the first half of this course, we break away from the typical MDS approach of using/interpreting data science software and start digging into the implementation of such software. If you don't like this, please keep in mind that it's a small part of the program. Also, I find it hard to imagine that the concepts learned here will turn out to be completely useless. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import scipy.optimize as spo\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (optional) Exercise 0: gradients of mathematical functions\n",
    "rubric={raw:7}\n",
    "\n",
    "Compute the gradient of each of the following mathematical functions. The notation for the gradient of a function $f$ is $\\nabla f(x)$. Note that $x$ may be a vector but $f$ returns a scalar in all the cases below. The gradient is also a vector.\n",
    "\n",
    "In some cases, the dimensionality of $x$ is provided: for example\n",
    "\n",
    "> $x \\in \\mathbb{R}^3$\n",
    "\n",
    "means that $x$ is a vector with 3 elements. In other cases, you should be able to infer the dimension from the context (for example, for $f_2$ we can infer that $x \\in \\mathbb{R}^2$ since otherwise the matrix multiplication wouldn't make sense). Finally, in some cases (like $f_6$) the dimension is unknown but you should be able to give an answer that holds regardless of the dimension of $x$. \n",
    "\n",
    "Hint: for $\\nabla f_5(x)$ you can write $x^\\top A x$ as a sum of a few terms. Try taking the derivative, and then putting it back into vector notation at the end.\n",
    "\n",
    "1. $f_1(x) = \\sin(x)$ where $x\\in \\mathbb{R}$\n",
    "2. $f_2(x) = [0\\;\\; 1]x$\n",
    "3. $f_3(x) = \\exp(x_1 + x_2x_3)$ where $x \\in \\mathbb{R}^3$\n",
    "4. $f_4(x) = \\exp(x_1 + x_2x_3)$ where $x \\in \\mathbb{R}^4$\n",
    "5. $f_5(x) = x^\\top A x$ where $A=\\left[ \\begin{array}{cc}1 & 2 \\\\0 & -3 \\end{array} \\right]$\n",
    "6. $f_6(x) = x^\\top x$\n",
    "7. $f_7(x) = x_1^2\\sin(x_2)$ where $x \\in \\mathbb{R}^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: gradients of Python functions\n",
    "\n",
    "Write a Python function that computes the gradient of each of the following Python functions. \n",
    "\n",
    "Notes: \n",
    "\n",
    "- All of the functions we deal with here return a scalar, regardless of the size of the input vector `x`. We will not consider the case where the output itself is a vector, since it is not relevant for our context of loss function minimization. \n",
    "- You do not need to consider the case where `x.ndim` is 2 (or higher); assume `x.ndim` is always 1.\n",
    "- You can use [scipy gradient checker](http://docs.scipy.org/doc/scipy-0.17.0/reference/generated/scipy.optimize.check_grad.html) to check your results for a few values of the inputs, as shown in the example below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success (probably)\n"
     ]
    }
   ],
   "source": [
    "# EXAMPLE\n",
    "def example(x):\n",
    "    return np.sum(x**2)\n",
    "\n",
    "def example_grad(x):\n",
    "    return 2.0*x\n",
    "\n",
    "def check_grad_and_print(fun, fun_grad, dims=5):\n",
    "    x0 = np.random.rand(dims)\n",
    "    diff = spo.check_grad(fun, fun_grad, np.array(x0))\n",
    "    if diff < 1e-5:\n",
    "        print('Success (probably)')\n",
    "    else:\n",
    "        print('Gradient incorrect (probably)')\n",
    "        \n",
    "check_grad_and_print(example, example_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1(a)\n",
    "rubric={accuracy:5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def foo(x):\n",
    "    return np.sum(x)\n",
    "\n",
    "def foo_grad(x):\n",
    "    pass # TODO\n",
    "\n",
    "check_grad_and_print(foo, foo_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1(b)\n",
    "rubric={accuracy:5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pin(x):\n",
    "    return np.sin(x[1])\n",
    "\n",
    "def pin_grad(x):\n",
    "    pass # TODO\n",
    "\n",
    "check_grad_and_print(pin, pin_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1(c)\n",
    "rubric={accuracy:5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.rand(100)\n",
    "\n",
    "def zap(x):\n",
    "    return w@x # this is matrix multiplication; equivalent to %*% in R.\n",
    "\n",
    "def zap_grad(x):\n",
    "    pass # TODO\n",
    "\n",
    "check_grad_and_print(zap, zap_grad, dims=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (optional) 1(d)\n",
    "rubric={accuracy:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baz(x):\n",
    "    result = 0\n",
    "    for i in range(len(x)):\n",
    "        result = result + x[i]**i\n",
    "    return result\n",
    "\n",
    "def baz_grad(x):\n",
    "    pass # TODO\n",
    "\n",
    "check_grad_and_print(baz, baz_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (optional) 1(e)\n",
    "rubric={accuracy:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "### OPTIONAL ###\n",
    "################\n",
    "\n",
    "def bar(x):\n",
    "    if np.abs(x[1]) > 2:\n",
    "        return 0\n",
    "    else:\n",
    "        return -(x[0]*x[0]+1)*np.cos(x[1]-1)\n",
    "\n",
    "def bar_grad(x):\n",
    "    pass # TODO\n",
    "    \n",
    "check_grad_and_print(bar, bar_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: gradient descent\n",
    "rubric={accuracy:10,quality:10}\n",
    "\n",
    "Write a function `gradient_descent` that takes in a function `f`, its gradient `f_grad`, and a starting point `x0`, and returns a minimizer of `f`.\n",
    "\n",
    "The code below tests your function by comparing the results to [`scipy.optimize.minimize`](http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, f_grad, x0, α=0.001, ϵ=0.01):\n",
    "    \"\"\"DOCSTRING GOES HERE\"\"\"\n",
    "    x = x0\n",
    "    \n",
    "    n = 0\n",
    "    while np.linalg.norm(f_grad(x)) > ϵ:        \n",
    "        # YOUR CODE GOES HERE (hint: it's not a lot of code!)\n",
    "        \n",
    "        n += 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing, using a function from Exercise 1b\n",
    "print(gradient_descent(pin, pin_grad, np.zeros(2)))\n",
    "print(spo.minimize(pin, np.zeros(2), jac=pin_grad).x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: logistic regression \n",
    "rubric={accuracy:20,quality:20,reasoning:10}\n",
    "\n",
    "The code below loads the IMDB dataset used in DSCI 571, with positive reviews encoded as $y=+1$ and negative reviews encoded as $y=-1$. You'll need the file `imdb_master.csv` in the current directory, or you can modify the path in `read_csv` below to where you have the file stored. As a reminder, **please do not commit/push this file**. I have attempted to seed your repos with a `.gitignore` file to this effect, but I'm not sure if it will work... let's see!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_df = pd.read_csv('imdb_master.csv', encoding = \"ISO-8859-1\")\n",
    "\n",
    "# Only keep the reviews with pos and neg labels\n",
    "imdb_df = imdb_df[imdb_df['label'].str.startswith(('pos','neg'))]\n",
    "\n",
    "# Train/test split\n",
    "imdb_df_train = imdb_df[imdb_df[\"type\"] == \"train\"]\n",
    "imdb_df_test  = imdb_df[imdb_df[\"type\"] == \"test\"]\n",
    "\n",
    "# Sample 5000 rows from the dataframe. \n",
    "imdb_df_subset_train = imdb_df_train.sample(n=5000, random_state=0)\n",
    "imdb_df_subset_test  = imdb_df_test.sample(n=5000, random_state=0)\n",
    "\n",
    "# Vectorizer\n",
    "movie_vec = CountVectorizer(max_features=200, \n",
    "                            stop_words='english', \n",
    "                            binary = True)\n",
    "movie_vec.fit(imdb_df_subset_train['review'])\n",
    "\n",
    "# Create X and y\n",
    "X_train = movie_vec.transform(imdb_df_subset_train['review']).toarray() \n",
    "y_train = (imdb_df_subset_train.label.values == \"pos\")*2-1\n",
    "\n",
    "X_test = movie_vec.transform(imdb_df_subset_test['review']).toarray() \n",
    "y_test = (imdb_df_subset_test.label.values == \"pos\")*2-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you'll implement a logistic regression classifier \"from scratch\". You'll implement the `fit` function using [`scipy.optimize.minimize`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html), which does something similar to your `gradient_descent` function above (but fancier). You should pass in the gradient using the `jac` argument as shown earlier in the lab.\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Complete the `fit` function.\n",
    "2. Report your training and test error. \n",
    "3. Report a confusion matrix for both train and test.\n",
    "4. Find one false positive case and one false negative case in the test set. Print out the corresponding reviews. Does the model trip up on reviews that seem more neutral? Or can you explain the results in some other way?\n",
    "5. (optional) Can you get the code to work using your `gradient_descent` function instead of `scipy.optimize.minimize`? This may be a pain because your `gradient_descent` probably uses a constant learning rate, and it's hard to pick the learning rate. Superior methods use a learning rate that is picked adaptively at every iteration. I haven't tried it yet on this data set but I'm happy to help you if you're interested.\n",
    "\n",
    "Notes: \n",
    "\n",
    "- There's no \"intercept\". This is just for simplicity. It is easy to add in a way that will shortly be explained in DSCI 573.\n",
    "- I suggest initializing your weights to all zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_lr(w, X, y):\n",
    "    return np.sum(np.log(1 + np.exp(-y*(X@w))))\n",
    "\n",
    "def loss_lr_grad(w, X, y):\n",
    "    return -X.T @ (y/(1+np.exp(y*(X@w))))\n",
    "\n",
    "class MyLogisticRegression:\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.sign(X@self.w)\n",
    "\n",
    "    # Fits the regression coefficients for a logistic regression model given the data X, y\n",
    "    def fit(self, X, y):\n",
    "        # YOUR CODE HERE (hint: it's not a lot of code!)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check gradient of loss\n",
    "grad_checker = spo.check_grad(lambda w: loss_lr(w, X_train, y_train), \n",
    "                              lambda w: loss_lr_grad(w, X_train, y_train), \n",
    "                              np.zeros(X_train.shape[1]))\n",
    "print(grad_checker, \"<-- should be small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: scikit-learn logistic regression\n",
    "rubric={reasoning:15}\n",
    "\n",
    "The scikit-learn implementation of logistic regression, which we looked at in DSCI 571, can be found [here](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). Compare your implementation to the sklearn one, both in terms of speed and accuracy, on the same problem as above. For a fair comparison, use the following hyperparameters with scikit-learn's `LogisticRegression`:\n",
    "\n",
    "- `C=1e8` to (mostly) disable regularization for sklearn, since your implementation doesn't use regularization. \n",
    "- `fit_intercept=False` since your code above doesn't fit an intercept term.\n",
    "- `solver=\"lbfgs\"` since that will become the new default in scikit-learn v0.22, and that's probably what `minimize` is using in your code anyway."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
