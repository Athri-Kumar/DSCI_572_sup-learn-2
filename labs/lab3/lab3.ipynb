{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI 572 Lab 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installations\n",
    "\n",
    "You'll first need to install Keras and the TensorFlow backend, which should be possible with\n",
    "\n",
    "```\n",
    "pip install tensorflow\n",
    "pip install keras\n",
    "```\n",
    "\n",
    "(I believe `conda` may also work for one or both packages.)\n",
    "\n",
    "You can check that it's working by importing Keras below. If all is well, it should import successfully and print \n",
    "```\n",
    "Using TensorFlow backend.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras imports\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.utils import np_utils\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "rubric={mechanics:10}\n",
    "\n",
    "Follow the [general lab instructions](https://ubc-mds.github.io/resources_pages/general_lab_instructions/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 0: neural networks \"by hand\"\n",
    "rubric={reasoning:10}\n",
    "\n",
    "Suppose that we use a neural network with one hidden layer and ReLU activations for a regression problem. After training, we obtain the following parameters:\n",
    "\n",
    "$$\\begin{align}W^{(0)} &= \\begin{bmatrix}-2 & 2 & -1\\\\-1 & -2 & 0\\end{bmatrix},  &b^{(0)}&=\\begin{bmatrix}2 \\\\ 0\\end{bmatrix} \\\\ W^{(1)} &= \\begin{bmatrix}3 & 1\\end{bmatrix},  &b^{(1)}&=-10\\end{align}$$\n",
    "\n",
    "For a training example with features $x = \\begin{bmatrix}3 \\\\-2 \\\\ 2\\end{bmatrix}$ what are the values in this network of $x^{(1)}$ and $\\hat{y}$? Show your work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: digits warm-up\n",
    "\n",
    "The code below loads scikit-learn's built-in handwritten digits dataset and fits the following classifiers:\n",
    "\n",
    "- KNN\n",
    "- random forest\n",
    "- RBF SVM\n",
    "- logistic regression\n",
    "- 1-hidden-layer neural network using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1347 train samples\n",
      "450 valid samples\n"
     ]
    }
   ],
   "source": [
    "digits = datasets.load_digits()\n",
    "X, y = digits['data'], digits['target']\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=1)\n",
    "\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_valid.shape[0], 'valid samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting knn\n",
      "Fitting random forest\n",
      "Fitting SVM\n",
      "Fitting logistic reg\n",
      "Fitting sklearn NN\n"
     ]
    }
   ],
   "source": [
    "classifiers = {\n",
    "    'knn'           : KNeighborsClassifier(),\n",
    "    'random forest' : RandomForestClassifier(n_estimators=50),\n",
    "    'SVM'           : SVC(C=100, gamma=\"scale\"),\n",
    "    'logistic reg'  : LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\"),\n",
    "    'sklearn NN'    : MLPClassifier() \n",
    "}\n",
    "\n",
    "train_scores = dict()\n",
    "valid_scores = dict()\n",
    "training_times = dict()\n",
    "\n",
    "for classifier_name, classifier_obj in classifiers.items():\n",
    "    print(\"Fitting\", classifier_name)\n",
    "    t = time.time()\n",
    "    classifier_obj.fit(X_train, y_train)\n",
    "    \n",
    "    training_times[classifier_name] = time.time() - t\n",
    "    train_scores[classifier_name] = classifier_obj.score(X_train, y_train)\n",
    "    valid_scores[classifier_name] = classifier_obj.score(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train acc</th>\n",
       "      <th>valid acc</th>\n",
       "      <th>training time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random forest</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logistic reg</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sklearn NN</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.98</td>\n",
       "      <td>1.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               train acc  valid acc  training time (s)\n",
       "knn                 1.00       0.98               0.10\n",
       "random forest       0.99       0.98               0.01\n",
       "SVM                 1.00       0.98               0.13\n",
       "logistic reg        1.00       0.98               0.17\n",
       "sklearn NN          1.00       0.98               1.60"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.float_format = '{:,.2f}'.format # make things look prettier when printing\n",
    "data = {\"train acc\": train_scores, \"valid acc\" : valid_scores, \"training time (s)\" : training_times}\n",
    "df = pd.DataFrame(data, columns=data.keys())\n",
    "df.index = list(classifiers.keys())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this dataset isn't very exciting in the sense that most of the methods get a high test accuracy after very little time. We'll use this dataset for a few more moments, and then move onto another one soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1(a)\n",
    "rubric={accuracy:5}\n",
    "\n",
    "Using Keras, create a neural network with the same architecture as the sklearn NN above. You'll need to deal with one-hot encoding of the labels, since this is how Keras expects the data. Briefly discuss your results.\n",
    "\n",
    "Warning: it has happened to students in the past that Keras caused their system to hang (not sure why). If this happens, I suggest (1) moving the Keras code outside of Jupyter into a regular `.py` file and (2) saving your work frequently. (But hopefully this was a known bug that has been fixed in the last year and won't be an issue anymore.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1(b)\n",
    "rubric={viz:5}\n",
    "\n",
    "For the same network above, make the following two plots:\n",
    "\n",
    "1. A plot of accuracy vs. optimization epochs. You should have two curves on this plot, one of train and one for test.\n",
    "2. A plot of the loss vs. optimization epochs. You should have two curves on this plot, one of train and one for test.\n",
    "\n",
    "Some notes:\n",
    "\n",
    "- To get access to this information, you can use the history object that is returned by `fit`. \n",
    "\n",
    "- If you're wondering what the difference is, accuracy is the percentage of examples that are correctles classified (always between $0$ and $1$), whereas the loss is literally the function being optimized (like `loss_lr` in lab 1, not necessarily between $0$ and $1$). The loss can't just be the number of incorrectly classifier examples, since this isn't a continuous function and would be too hard to optimize. More on this coming in DSCI 573. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1(c)\n",
    "rubric={reasoning:5,quality:5}\n",
    "\n",
    "The optimization problem of training a neural network is non-convex. To explore this, try training\n",
    "your network several times. You will get different results due to different random initializations and different randomness in the optimization method itself. Explore how the training/validation error changes across\n",
    "runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (optional) 1(d) \n",
    "rubric={reasoning:1}\n",
    "\n",
    "Continuing with the above, try also exploring how the model parameters themselves (the weights) change across runs (you'll need to think of a reasonable way of comparing the weights of two networks).\n",
    "Do you observe substantially different weight sets all with a similar prediction accuracy... or something different?\n",
    "\n",
    "Note: to inspect the weights themselves, use the `get_weights()` function for the Keras model/layer object. I also provide a function, below, that grabs all the weights of a trained model and puts them in one big flat numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_all_weights = lambda model: np.concatenate(tuple(map(lambda wi: wi.flatten(), model.get_weights())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: big digits\n",
    "\n",
    "Next, we'll move on to the famous the MNIST digits -- a classic dataset for deep learning. The MNIST data set is  bigger than the digits dataset built into sklearn: the images are larger ($28\\times28$ instead of $8\\times8$) and there are more of them ($70000$ insetad of $1797$). In total, we're dealing with $70000\\times28\\times28\\approx 55$ million training pixels instead of $1797\\times8\\times8\\approx80000$ training pixels (about $500$ times more data). \n",
    "\n",
    "If you're interested, check out [this site](http://yann.lecun.com/exdb/mnist/) showing the progress on this dataset from 1998 to 2012. \n",
    "\n",
    "The following code loads the MNIST dataset. The first time you run it, the data will be downloaded. In future times, it will use the local version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data, shuffled and split between train and test sets\n",
    "(X_train_img, y_train), (X_test_img, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display a random training example\n",
    "i = np.random.randint(0,len(X_train_img))\n",
    "plt.imshow(X_train_img[i],cmap='gray')\n",
    "plt.title('This is a %d' % y_train[i]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train_img.reshape(60000, 28*28)\n",
    "X_test = X_test_img.reshape(10000, 28*28)\n",
    "X_train = X_train / 255 # this is the same a scaling, since the pixel intensities range from 0 to 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2(a)\n",
    "rubric={reasoning:15}\n",
    "\n",
    "1. Which of the classifiers from Exercise 1 scale well to this larger data set? You are free to make arguments using theory (big-O running times) and/or experiment (timed runs) as you see fit. Keep in mind that we've increased both $n$ (number of examples) and $d$ (number of features). Don't subject yourself to experiments that take tens of minutes or hours. It is fine to declare defeat after a couple of minutes and say that a method doesn't scale. But, when that happens, try to say a little bit about why that might be the case. \n",
    "2. For those methods where the running time is reasonable (say, a couple minutes of computation), how does the accuracy compare between the methods?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2(b)\n",
    "rubric={reasoning:15}\n",
    "\n",
    "The code below runs a bigger Keras model on the full MNIST data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 7s 109us/step - loss: 0.2633 - acc: 0.9231 - val_loss: 0.1293 - val_acc: 0.9618\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0978 - acc: 0.9710 - val_loss: 0.0899 - val_acc: 0.9707\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0627 - acc: 0.9803 - val_loss: 0.0738 - val_acc: 0.9760\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 7s 109us/step - loss: 0.0460 - acc: 0.9849 - val_loss: 0.0766 - val_acc: 0.9755\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 7s 110us/step - loss: 0.0326 - acc: 0.9891 - val_loss: 0.0859 - val_acc: 0.9740\n",
      "Train accuracy: 0.9891333333333333\n",
      "Test accuracy: 0.974\n",
      "---Running Time: 37.09114193916321 seconds ---\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 5\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, num_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size, epochs=epochs,\n",
    "                    verbose=1, validation_data=(X_test, Y_test))\n",
    "\n",
    "score = model.evaluate(X_train, Y_train, verbose=0)\n",
    "print('Train accuracy:', score[1])\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "elapsed_time = time.time()-start_time\n",
    "print(\"---Running Time: %s seconds ---\" % elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the neural network implemented with Keras above, explore the effects of the different hyperparameters on accuracy. Try at least 3 variations on what you're given above. In each case, briefly discuss your results. Some things you can consider trying:\n",
    "  - adding/removing layer(s)\n",
    "  - changing the [activation functions](https://keras.io/activations/) from `relu` to `tanh`\n",
    "  - adding [regularization](https://keras.io/regularizers/) such as dropout\n",
    "  - changing the way the weights are [initialized](https://keras.io/initializations/)\n",
    "  - changing the [optimizer](https://keras.io/optimizers/) from adam to something else, like SGD. Read the documentation and try changing the _hyperparameters of the optimizer_ in addition to just the type of optimizer (e.g., $\\alpha$ in gradient descent is a hyperparameter of the optimizer). It should not be difficult to completely mess up your training procedure, for example by tampering with the learning rate.\n",
    "\n",
    "**NOTE: if at any point things are just way too slow, you can use a subset of the data to speed things up. But try to still draw interesting conclusions to the extent possible.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: pondering neural networks\n",
    "rubric={reasoning:15}\n",
    "\n",
    "1. Explain why a 1-layer (zero-hidden-layer) neural network with a linear activation is equivalent to linear regression. If you just wanted to do linear regression, what are the disadvantages of using Keras instead of a package like R's `lm`?\n",
    "2. Consider a regression problem with $d=500$ features. You use a 2-hidden-layer neural network with hidden layer sizes $100$ and $200$. How many parameters does the model have? Justify your answer.\n",
    "3. Name one advantage and one disadvantage of stochastic gradient descent, when comparing it to gradient descent?\n",
    "4. Hyperparameter optimization would be easy if we could independently tune each hyperparameter. Then, with $m$ hyperparameters, we'd just have $m$ one-dimensional optimization problems. However, in reality the hyperparameters interact, which leaves you with the more daunting $m$-dimensional optimization problem. Consider for example a regularization hyperparameter (like dropout) and an architecture parameter (like the number of units or layers). Why do you think these hyperparameters would \"interact\" with each other? What combinations do you think might lead to better or worse performance? \n",
    "5. If you look at the [documentation for scikit-learn's neural network classifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html), you'll see that the user only needs to input the sizes of the hidden layers; like other scikit-learn classifiers, there's no need for the user to explicitly enter the number of features when creating the model object. However, Keras requires us to set the `input_dim` argument in the first `Dense` layer, which specifies the number of features. Why do you think Keras requires this whereas scikit-learn doesn't?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise 4\n",
    "rubric={reasoning:5}\n",
    "\n",
    "It should be the case that logistic regression is equivalent to a neural network with no hidden layers. Using sklearn's `MLPClassifier`, fit a zero-hidden-layer neural network on the synthetic 2D data (generated below) and compare the learned weights to what you get with `LogisticRegression`. Do you get essentially the same model with the neural network? You'll need to fiddle with the hyperparameters (optimization, regularization) to get something equivalent. \n",
    "\n",
    "If you want to be very careful, see if you can get the coefficients to be exactly the same up to several decimal places (I managed to do this).  However, this might not be a good use of your time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "d = 2\n",
    "X = np.random.rand(n,2)\n",
    "y = np.random.rand(n) > 0.7\n",
    "X[y==0,0] += .7\n",
    "X[y==0,1] += .3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (optional) Exercise 4(b)\n",
    "rubric={reasoning:1}\n",
    "\n",
    "Try to get the above working with Keras as well. I tried for about 20 minutes and wasn't successful (speaking of questionable uses of time...), so if you figure it out please send me a message and I'll add your solution to the official solutions. Some reasons why this is annoying:\n",
    "\n",
    "- One has to be very careful with the (L2) regularization. The interpretation of the regularization strength may vary across packages. It might be easier to turn off regularization for both.\n",
    "- It's not clear whether to use a `softmax` activation + `categorical_crossentropy` loss + one-hot enoded labels or `sigmoid` activation + `binary_crossentropy` loss + regular labels. I have slightly more faith in the former approach. For this reason, it might be easier to first try and get this working with linear regression, and then move on to logistic.\n",
    "- The SGD-based optimizers available in Keras aren't well suited to this kind of toy dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (optional) Exercise 5: backprop\n",
    "rubric={reasoning:1}\n",
    "\n",
    "From scratch using only raw numpy, implement a one-hidden-layer neural network for regression using ReLUs. Show your computation of the gradients alongside the code. You can ignore the fact that the ReLU function is not differentiable when its input equals zero.\n",
    "\n",
    "NOTE: there are probably a lot of resources out there where people give their \"raw\" neural network implementations. If you're going to do this and you consult any sources, make sure you cite them. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
